
  {
    "name": "treatment-sql-data-ready-couchbase-source-connector1",
    "config": {
        "connector.class": "com.couchbase.connect.kafka.CouchbaseSourceConnector",
        "tasks.max": "1",
        "couchbase.topic": "path.treatment.sql.data",
        "couchbase.seed.nodes": "couchbases://cb.wf6ax-4hcobxv2w.cloud.couchbase.com",
        "couchbase.bootstrap.timeout": "60s",
        "couchbase.bucket": "Path_Lite_Dev3",
        "couchbase.collections": "treatment.Encounter",
       "couchbase.enable.tls":"true",
       "couchbase.trust.certificate.path":"C:\\Temp\\kafka_2.13-3.9.1\\config\\certs\\PATH-DEV-root-certificate.txt",
        "couchbase.username": "mt-dev",
        "couchbase.password": "D@v!t@2025",
        //custom fikter for events 
        "couchbase.event.filter":"com.couchbase.connect.kafka.example.CustomFilter",
        "couchbase.custom.filter.rocksdb.path":"C:/Temp/rocksdb_store",
        
        "couchbase.source.handler": "com.couchbase.connect.kafka.handler.source.RawJsonSourceHandler",
        "key.converter": "org.apache.kafka.connect.storage.StringConverter",
        "value.converter": "org.apache.kafka.connect.json.JsonConverter",
        "value.converter.schemas.enable":"false",
        "couchbase.stream.from": "SAVED_OFFSET_OR_BEGINNING",
        "couchbase.compression": "ENABLED",
        "couchbase.flow.control.buffer": "64m",
        "couchbase.persistence.polling.interval": "100ms",
        //configuration for custom tranforms
         "transforms": "filterDocs,dropFields",
         "transforms.filterDocs.type": "com.example.kafka.connect.transforms.ConditionalDocumentFilter",
         "transforms.filterDocs.couchbase.conditional.filter.expr": "id != null && (txStatusTracker == null || ( txStatusTracker.current.value != null && txStatusTracker.current.value == 'none'))",
         //configuration to drop the feilds ( couchbase inbuilt SMT only accepts MAP)
        "transforms.dropFields.type":"org.apache.kafka.connect.transforms.ReplaceField$Value",
        "transforms.dropFields.blacklist":"txStatusTracker,practitioner,hospital,serviceType,demographicsAndAdmission,order,txWaitTimeList,preTxDetails,patientEducation,timeoutSafteryCheck,NonTxService,hdIntraTreatment,aphresisIntraTreatment,pdIntraTreatment,aphersisIntraTreatment,pdIntraTreatment,crrIntraTreatment,postTxDetails,reviewSubmit,statusRecord,acoiData,billingOnly,txStatusTracker,syncStatusTracker,_partition,deviceMeta,enteredInError,parentReference,reasonType,_modifiedBy",
        "errors.tolerance": "all",
        "errors.log.enable": "true",
        "errors.log.include.messages": "true"
    }
}

Couchbase DCP stream reads from treatment.Encounter.

CustomFilter filters documents by "type": "order".

ConditionalDocumentFilter applies logical expression to refine further.

ReplaceField SMT removes unwanted fields.

RawJsonSourceHandler emits final JSON to Kafka.

Kafka topic path.treatment.sql.data receives clean, filtered documents.


Data Source (Couchbase)

Connector reads documents from Path_Lite_Dev3 → treatment.Encounter collection.

TLS is enabled using root cert.

Custom Event Filter (Java + RocksDB)

Each document is hashed.

RocksDB stores previous hash values per document ID.

If hash is same → skip event (duplicate).

If new or changed → forward to Kafka.

Special cases:

id missing → skipped (invalid doc).

First insert (not in RocksDB) → allowed.

Transforms (Kafka SMTs)

filterDocs: Uses expression filter → only passes docs with id != null and txStatusTracker == 'none' or null.

dropFields: Removes sensitive/unnecessary fields before writing to Kafka.

Kafka Topic Publishing

Final JSON written into topic: path.treatment.sql.data.

Consumers can now pick up a clean, deduplicated, filtered event stream.